#!/usr/bin/env python
# coding: utf-8

# # Project Details

# # Project 1 - Migration of application's legacy file systems data to RDBMS.

# #### Brief Background:
# With most of the expert mainframe staff expressing their desire to retire in the next few years and organization's looking forward to reducing cost of maintenance and infrastructure, there is an increase in demand for modernization of mainframe applications.
# There was a need to migrate the application's legacy files called as VSAM files to IBM DB2. The interaction between batch, online, data exchange, writing queries in SQL, data manipulation and extraction of data from RDBMS such as IBM DB2 is much more efficient than legacy files such as VSAM.  At the same time, making it available for languages other than mainframes.
# 
# 

# #### Roles and Responsibilities:

# 
# 1.Analyzed the VSAM files that had customer information and customer loyalty program related information in detail with the help of copybooks that define the structure of the data.
# 

# 2.Logical data model created.
# 

# 3.Assisted the development teams to create an ETL process that extracts the data from Vsam, Transforms the data and load the data to the table.
# 

# 4.Performed validation and verification of the data conversion based on the various mappings while taking care of EBCDIC to ASCII formats and other formats specific to mainframe.

# # Project 2 - Rewards clean-up
# 

# #### Brief Background:
# Clean-up of old rewards products for US Bank by
# identification, analysis, reporting and visualization of data. This was done to reduce the cost charged by the third party to the  bank for each active record present.

# #### Roles and Responsibilities:

# 1.Identified the products which are not eligible to earn the reward points/old products for the cleanup. Analysis was done using Python pandas and SQL.
# Phase1: Account level cleanup is done in which 690k  customers were closed in rewards. Phase2: Customer level cleanup is done in which around 590k customers were closed in rewards system.

# 2.Reports created for the LOB using SAS for verification.

# 3.Dashboards created in Microsoft Power BI for visualisation of data.

# # Project 3 - SOP automation
# 

# #### Brief background:
# Many Standard operating procedures have to be perfomed for BAU activities in Banking and Payments domain. These are mostly recurring activities which required manual efforts.

# #### Roles and Responsibilities:

# 1.Analysed the need for automation as some tasks aren't suited for automation that require a degree of creativity, flexibility that automatic systems can't provide or for more complicated or less frequently executed tasks creating the automation may actually be more effort or cost than its worth.

# 2.After completing understanding the need and recieving approvals from the client, developed modules in SAS that took care of SOP's with no manual intervention leading to increase in efficiency and decrease in cost and also made way for the utilisation of the key resources time in other important BAU activities.

# # Project 4 - RCA reports to address problems related to Banking and cards
# 

# #### Brief Background:
# The increase in failures related to customer enrolment, card being lost/stolen/misplaced, data sync issues with third party vendors posed greater problem as dedicated staff was required to look into each of problem and provide solutions.

# #### Roles and responsibilities:
# The data was collected and analysed in Microsoft Excel, RCA Reports were created and shared with the LOB and respective development team to fix the bugs in the system that was leading to these errors.

# # Project 5 - Basic data warehouse solution using Airflow
# 

# #### Brief Background:
# There was a requirement of building a data warehouse solution capable of doing a set of data processing tasks.

# #### Roles and Responsibilities:

# 1.To Load CSV / other format files to data warehouse. Files included sensitive data for providing disaster relief to customers.

# 2.To handle the data transformation process and then loading to the table which can be used by data analytics team for data visualization etc.

# 3.Assisted the data engineers in defining DAG arguments and the DAG, creating tasks to download data, extract data from files of different formats, consolidate data extracted from previous tasks, transform the data, define the task pipeline.

# 4.Perform verification and validation of transformed data.
